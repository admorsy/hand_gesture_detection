{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/admorsy/hand_gesture_detection/blob/main/gesture_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing modules and packages needed for code"
      ],
      "metadata": {
        "id": "B7FGYcOSKj-o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUzGj1uU1x4P"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe==0.10.21 mediapipe-model-maker==0.2.1.4 numpy==1.23.5 opencv-python==4.11.0.86 scikit-learn==1.6.1 tensorflow==2.15.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating, training, and evaluating the custom model"
      ],
      "metadata": {
        "id": "LsP2OoS4KvQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mediapipe_model_maker import gesture_recognizer\n",
        "\n",
        "# Load dataset from your ASL gesture dataset\n",
        "data = gesture_recognizer.Dataset.from_folder(dirname=\"/content/drive/MyDrive/Colab_Notebooks/asl_dataset\")\n",
        "\n",
        "# Split dataset into 80% training, 20% validation\n",
        "train_data, validation_data = data.split(0.8)\n",
        "\n",
        "# Define options for the Gesture Recognizer\n",
        "options = gesture_recognizer.GestureRecognizerOptions(\n",
        "    hparams=gesture_recognizer.HParams(export_dir=\"/content/drive/MyDrive/Colab_Notebooks/exported_model\")\n",
        ")\n",
        "\n",
        "# Create and train the model\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options  # Pass the GestureRecognizerOptions object\n",
        ")\n",
        "\n",
        "print(\"ðŸŽ‰ Training complete! The model has been saved.\")\n",
        "\n",
        "# Evaluate the model\n",
        "metric = model.evaluate(validation_data)\n",
        "print(\"Model Evaluation:\", metric)\n",
        "\n",
        "# Export the trained model\n",
        "model.export_model(model_name=\"sign2number.task\")"
      ],
      "metadata": {
        "id": "g1V2q0sJXiEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is responsible for opening the webcam to detect hand gestures.\n",
        "\n",
        "The detected gesture is then recognized by gesture_recognizer, and is converted to text, in the corner of the screen, that reresents the recognized gesture."
      ],
      "metadata": {
        "id": "iJimdmMILFMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks.python.vision import GestureRecognizer, GestureRecognizerOptions\n",
        "model_path = \"/content/drive/MyDrive/Colab_Notebooks/exported_model/sign2number.task\"\n",
        "\n",
        "\n",
        "# Initialize MediaPipe Hands for landmark detection\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands( static_image_mode=False,\n",
        "    model_complexity=1,\n",
        "    min_detection_confidence=0.75,\n",
        "    min_tracking_confidence=0.75,\n",
        "    max_num_hands=1)\n",
        "\n",
        "# Load the trained gesture recognition model\n",
        "options = GestureRecognizerOptions(\n",
        "    base_options=mp.tasks.BaseOptions(model_asset_path=model_path)\n",
        ")\n",
        "recognizer = GestureRecognizer.create_from_options(options)\n",
        "\n",
        "# Open webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Flip the frame horizontally (mirror effect)\n",
        "    frame = cv2.flip(frame, 1)\n",
        "\n",
        "    # Convert frame to RGB for gesture recognition\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process hand landmarks\n",
        "    results = hands.process(rgb_frame)\n",
        "\n",
        "    # Convert the frame to grayscale\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Convert grayscale back to BGR so it can display with colored landmarks\n",
        "    gray_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Draw hand landmarks if detected (on grayscale frame)\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            mp_drawing.draw_landmarks(\n",
        "                gray_frame,\n",
        "                hand_landmarks,\n",
        "                mp_hands.HAND_CONNECTIONS,\n",
        "                mp_drawing.DrawingSpec(color=(0, 100, 255), thickness=8, circle_radius=4),\n",
        "                mp_drawing.DrawingSpec(color=(255, 10, 255), thickness=4, circle_radius=4),\n",
        "            )\n",
        "\n",
        "    # Convert frame to MediaPipe Image for gesture recognition\n",
        "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "\n",
        "    # Run gesture recognition\n",
        "    recognition_result = recognizer.recognize(mp_image)\n",
        "\n",
        "     # Display recognized gestures on screen with a background\n",
        "    if recognition_result.gestures:\n",
        "        for gesture in recognition_result.gestures:\n",
        "            gesture_name = gesture[0].category_name  # Get the most confident gesture\n",
        "\n",
        "            # Define text properties\n",
        "            text = f\"{gesture_name}\"\n",
        "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            font_scale = 2\n",
        "            font_thickness = 3\n",
        "            text_color = (255, 255, 255)  # text color\n",
        "            bg_color = (255, 10, 255)  # background color\n",
        "\n",
        "            # Get text size\n",
        "            text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
        "            text_w, text_h = text_size\n",
        "\n",
        "            # Define text position\n",
        "            x, y = 10, 50  # Top-left corner\n",
        "\n",
        "            # Draw background rectangle\n",
        "            cv2.rectangle(gray_frame, (x - 10, y - text_h - 10), (x + text_w + 10, y + 10), bg_color, cv2.FILLED)\n",
        "\n",
        "            # Put text on top of the rectangle\n",
        "            cv2.putText(gray_frame, text, (x, y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
        "\n",
        "    # Show the grayscale webcam feed with colored landmarks and text background\n",
        "    cv2.imshow(\"Hand Tracking & Gesture Recognition\", gray_frame)\n",
        "\n",
        "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "# cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "7w-72gcuFNPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sQIpbw1OmjnVq-yTZwH9xogaaTfvQTCZ",
      "authorship_tag": "ABX9TyO4naTZkTnXP4onuTFuSmK/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}